# @package _group_

# model type. One of [hf_bert, pytext_bert, fairseq_roberta]
encoder_model_type: hf_any

# HuggingFace's config name for model initialization
pretrained_model_cfg: bert-base-uncased

# Some encoders need to be initialized from a file
pretrained_file:

# Extra linear layer on top of standard bert/roberta encoder
projection_dim: 0

# Whether to use mean pool or not.
mean_pool: False
mean_pool_ctx_only: False

# Whether to use factor rep.
factor_rep: False

# Whether to share encoders for query and context.
shared_encoder: False

# Max length of the encoder input sequence
sequence_length: 256

dropout: 0.1

# whether to fix (don't update) context encoder during training or not
fix_ctx_encoder: False

# whether to use title for context passages.
use_title: True

# whether to use moe.
use_moe: False
num_expert: 2
use_infer_expert: False
per_layer_gating: False
moe_type: mod3:fwd

# if False, the model won't load pre-trained BERT weights
pretrained: True
